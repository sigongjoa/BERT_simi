{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "model = BertModel.from_pretrained(\"kykim/bert-kor-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장 A</th>\n",
       "      <th>문장 B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>간은 피로회복에 중요하다.</td>\n",
       "      <td>신장은 노폐물 배출에 중요하다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>요가를 하면 마음이 안정된다.</td>\n",
       "      <td>스트레칭을 하면 유연해진다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>간을 맞으면 아프다.</td>\n",
       "      <td>턱을 맞으면 기절한다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>사과는 피부 미용에 좋다.</td>\n",
       "      <td>녹용은 간에 좋다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미나리는 피를 맑게한다.</td>\n",
       "      <td>인삼은 몸에 에너지를 충전한다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               문장 A               문장 B\n",
       "0    간은 피로회복에 중요하다.  신장은 노폐물 배출에 중요하다.\n",
       "1  요가를 하면 마음이 안정된다.    스트레칭을 하면 유연해진다.\n",
       "2       간을 맞으면 아프다.       턱을 맞으면 기절한다.\n",
       "3    사과는 피부 미용에 좋다.         녹용은 간에 좋다.\n",
       "4     미나리는 피를 맑게한다.  인삼은 몸에 에너지를 충전한다."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/BERT_sim_sents1.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT 모델을 사용해서 문장의 유사도 확인\n",
    "현재 bert의 per-train의 output을 mean_pooling을 이용해서 sent vector로 바꾼 후 유사도를 계산하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 문장씩 비교 하는 방법\n",
    "def word_sim(sent1 , sent2):\n",
    "    words = [sent1, sent2]\n",
    "    tokens = {'input_ids' : [] , 'attention_mask' : []}\n",
    "\n",
    "    \n",
    "    for word in words:\n",
    "        print(word)\n",
    "        print(\"token의 수 : \" +str(len(tokenizer.encode_plus(word)['input_ids'])))\n",
    "        new_tokens = tokenizer.encode_plus(word , max_length=30, truncation= True , padding='max_length' , return_tensors='pt')\n",
    "        \n",
    "        tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "        tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "    tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "    tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
    "    \n",
    "    outputs = model(**tokens)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    #return embeddings\n",
    "    attention_mask = tokens['attention_mask']\n",
    "    mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    masked_embeddings = embeddings * mask\n",
    "    summed = torch.sum(masked_embeddings,1)\n",
    "    summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "    mean_pooled = summed / summed_mask\n",
    "    mean_pooled = mean_pooled.detach().numpy()\n",
    "    return cosine_similarity( mean_pooled[0].reshape(1,-1) , mean_pooled[1].reshape(1,-1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "신장은 노폐물 배출에 중요하다.\n",
      "token의 수 : 9\n",
      "0번째의 문장의 유사도는 0.8338543176651001\n",
      "\n",
      "요가를 하면 마음이 안정된다.\n",
      "token의 수 : 9\n",
      "스트레칭을 하면 유연해진다.\n",
      "token의 수 : 8\n",
      "1번째의 문장의 유사도는 0.7455722689628601\n",
      "\n",
      "간을 맞으면 아프다.\n",
      "token의 수 : 6\n",
      "턱을 맞으면 기절한다.\n",
      "token의 수 : 8\n",
      "2번째의 문장의 유사도는 0.6222368478775024\n",
      "\n",
      "사과는 피부 미용에 좋다.\n",
      "token의 수 : 9\n",
      "녹용은 간에 좋다.\n",
      "token의 수 : 7\n",
      "3번째의 문장의 유사도는 0.7551013231277466\n",
      "\n",
      "미나리는 피를 맑게한다.\n",
      "token의 수 : 8\n",
      "인삼은 몸에 에너지를 충전한다.\n",
      "token의 수 : 9\n",
      "4번째의 문장의 유사도는 0.6823358535766602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(0,5):\n",
    "    sim = word_sim(data['문장 A'][idx],data['문장 B'][idx])\n",
    "    print(f'{idx}번째의 문장의 유사도는 {sim}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실험에 사용된 PC의 환경(연구실 컴퓨터)\n",
    "```\n",
    "프로세서\tIntel(R) Core(TM) i5-7500 CPU @ 3.40GHz   3.40 GHz\n",
    "설치된 RAM\t16.0GB(15.9GB 사용 가능)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer에서 실행 시간 확인\n",
    "tokenizer에서는 대략 1000 nano sec 시간 소요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_time(sent):\n",
    "    tokenizer.encode_plus(sent , max_length=10, truncation= True , padding='max_length' , return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 997 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "token_time(data['문장 A'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 두 문장의 유사도 실행 시간\n",
    "\n",
    "대략 125 ms 소요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "스트레칭을 하면 유연해진다.\n",
      "token의 수 : 8\n",
      "Wall time: 124 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5451042"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "word_sim(data['문장 A'][0],data['문장 B'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위 수행 시간 비교를 10번 반복 했을 때의 실행시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "간은 피로회복에 중요하다.\n",
      "token의 수 : 9\n",
      "Wall time: 977 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in range(0,9):\n",
    "    word_sim(data['문장 A'][0],data['문장 A'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모든 문장을 한번에 입력 하는 경우를 Test\n",
    "\n",
    "0.52s정도 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45982772 0.3141464  0.37281248 0.35410008]]\n",
      "[[0.45982772 0.3141464  0.37281248 0.35410008]]\n",
      "[[0.45982772 0.3141464  0.37281248 0.35410008]]\n",
      "[[0.45982772 0.3141464  0.37281248 0.35410008]]\n",
      "[[0.45982772 0.3141464  0.37281248 0.35410008]]\n",
      "[[0.45982772 0.3141464  0.37281248 0.35410008]]\n",
      "[[0.45982772 0.3141464  0.37281248 0.35410008]]\n",
      "[[0.45982772 0.3141464  0.37281248 0.35410008]]\n",
      "[[0.45982772 0.3141464  0.37281248 0.35410008]]\n",
      "[[0.45982772 0.3141464  0.37281248 0.35410008]]\n",
      "Wall time: 515 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# senst = ['상품을 표현하는 문장' , '에너지에 대한 문장' , '회복에 대한 문장' , '순환에 대한 문장' , '정화에 대한 문장']\n",
    "sents = ['차원을 설명하는 문장', data['문장 A'][1] , data['문장 A'][2] , data['문장 A'][3] , data['문장 A'][4]]\n",
    "add_sents = sents + sents + sents + sents + sents + sents + sents + sents + sents + sents\n",
    "tokens = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "for sentence in add_sents:\n",
    "    # encode each sentence and append to dictionary\n",
    "    new_tokens = tokenizer.encode_plus(sentence, max_length=10,\n",
    "                                       truncation=True, padding='max_length',\n",
    "                                       return_tensors='pt')\n",
    "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "\n",
    "# reformat list of tensors into single tensor\n",
    "tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
    "attention_mask = tokens['attention_mask']\n",
    "outputs = model(**tokens)\n",
    "embeddings = outputs.last_hidden_state\n",
    "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "masked_embeddings = embeddings * mask\n",
    "summed = torch.sum(masked_embeddings, 1)\n",
    "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "mean_pooled = summed / summed_mask\n",
    "mean_pooled = mean_pooled.detach().numpy()\n",
    "\n",
    "\n",
    "for idx in range(0,len(add_sents),5):\n",
    "    print(cosine_similarity([mean_pooled[idx]],mean_pooled[idx+1:idx+5]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f930088951e3dfac03a7eb812b8d23627d62fa3bb27ad45034a94c87625f03b9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
